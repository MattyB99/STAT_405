---
title: "Your Project Title"
author: "Matthew Barclay, Griffin Coccari, Alex Gallegos, Anushi Singhal"
output: pdf_document
#output: html_document
fontsize: 10pt
geometry: margin=1in
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
#library(usethis)
library(ggmap)
library(rgdal)
library(here)
library(proj4)
library(magrittr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(magick)
library(png)
library(ggimage)
library(mapdeck)
library(osmdata)
library(maps)
#library(sf)
library(grid)
library(gridBase)
library(knitr)

Parking_Citations <- readRDS(file = "Parking_Citations.rds")
```
### Introduction  
   Alex, Anushi, Matt and Griffin all come from different backrounds and majors, so in disucssions for which dataset we wanted to analyze, we explored a wide range of topics from public health to the New York City subway We eventually decided to take on data from another large metropolis, but this time in sunny Los Angeles. In Los Angeles, the beautiful beaches, countless celebrities, and hollywood homes are magical for visitors. However, LA traffic is often very bad and parking in tourist areas can be a nightmare. As the second largest city in the United States, there are over 6.4 million vehicles in the Los Angeles urbanized area^1^. Our dataset, Parking_Citations, contains all the details of nearly 10 million parking violations in Los Angeles from 2010 to the present. That means we have access to the records that account for fines totalling close to $600 million. With so much money at stake and the huge volume of data, the city of Los Angeles keeps track of these records electronically. We intend to take this massive amount of data and transform it so that the intricacies of parking violations can be easily understood.

```{r 3D Heatmap, echo=FALSE, message=FALSE, warning=FALSE}
if(knitr::is_html_output()){
set_token(Sys.getenv("MAPBOX"))
mapdeck(pitch = 45, zoom = 100) %>%
  add_grid(data = Parking_Citations[1:30000,], lat = "real_lat", lon = "real_lng", 
           cell_size = 1000, elevation_scale = 50, layer_id = "grid_layer",
           colour_range = viridisLite::plasma(6))} 
```

```{r 3D_Map, echo=FALSE, out.width="0.5\\linewidth", out.height="0.5\\linewidth", fig.align="center", message=FALSE, warning=FALSE}
if(!knitr::is_html_output()) {knitr::include_graphics("3D_Map.png") }
```

   
##### Big Data  
  The raw data was accessed directly from the City of Los Angeles Department of Transportation (LADOT) through the city's Open Data website^2^. The original data set contained 9.97 million rows, each containing details on one parking violation. We identified the fine amount and location data as the most important variables and therefore removed all rows containing empty or NA values in those two columns. This narrowed the amount of rows to about 8.5 million. LADOT uses US Feet coordinates according to the NAD_1983_StatePlane_California projection, which is not easily comparable to standard latitude and longitude values, so our next step was to use **sp** package for R to transform our location data. Our final cleaned data set condained data on 8,502,692 tickets, including information of the time, date, location, vehicle information, parking offense, and more. Although well over the 1 milion row minimum requirement, this large amount of data will allow us to explore the trends in parking violations in LA over a relatively large time frame.
   
   
***   
#### numer of fines per year

```{r, echo=FALSE, message=FALSE, warning=FALSE}


Parking_Citations%>% 
mutate(Year=format(as.Date(Issue.Date, format="%m/%d/%Y"), format=("%Y"))) ->Parking_Citations
Parking_Citations$Year <- as.integer(Parking_Citations$Year)
                                     
Parking_Citations %>%
  group_by(Year) %>%
  summarise(count=n())->df
x <- df$Year
y <- df$count


grid.newpage()
pushViewport(plotViewport(margins = c(5.1, 4.1, 4.1, 2.1)))
pushViewport(dataViewport(x, y))
grid.rect(gp=gpar(fill="yellow", alpha=.7))
grid.text(label="Parking Citations By Year",x=.5, y=1.05, gp=gpar(fontface="bold", cex=1.5))
grid.xaxis(at=x)
grid.yaxis(at=y, gp=gpar(cex=.8))
grid.lines(x,y, default.units = "native", gp=gpar(lty=8, col="red", lwd=3))
grid.points(x[1:7], y[1:7], pch=24, size=unit(0.025, "npc"), gp=gpar(fill="red"))
grid.points(x[8:11], y[8:11], pch = 25, size=unit(0.025, "npc"), gp=gpar(fill="green"))
popViewport(2)
```

When we began our analysis, we had to get some idea of what we were dealing with. With no instructions, We first boiled down the data into a more understandble form. The data gave us a good amount of information, and it also gave us a good starting point. The data showed a skewed distrubution over 11 year span. From 2010 to 2014 there were less than 40 parking citations recorded. In 2014 that number increased to 34,000. After that, we saw a drastic 44 percent change into 2015, when 1,541,535 parking citations were recorded. The count peaked at 2,154,321 citations in 2017. This peak accentuated both the steep rise leading up to 2017 and also the suprisingly steep decline afterwards. While this graph gave us a good handle on the numbers, there were questions unanswered about the data collection.The  drastic increase from 2014-2015 may be explained by an attempt by the City of Los Angeles to digitize their citations. But that explanation is contradicted by the equally sharp decline after 2017. This left the question of how many citations did they actually collect? The graph didn't tell us exactly, but it gives us a range of values that captures the true value or something close. Even without the actual answer, this graph highlighted the magnitude of the citaitons in LA.
```{r dollars per year, echo=FALSE, message=FALSE, warning=FALSE}
data.frame(Year=c(seq(from=2010, to=2020, by=1), "Total"),
         "Dollars_Collected" =c(730, 630, 2276, 3062, 2371400, 107172874, 130124585,151608139,124738695,75291586,2275, sum(c(730, 630, 2276, 3062, 2371400, 107172874, 130124585,151608139,124738695,75291586,2275))))%>%knitr::kable()

```

```{r cleanup, eval=FALSE, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
#will not run with eval = FALSE
locs <- Parking_Citations%>%filter(!is.na(Latitude), !is.na(Fine.amount), Latitude > 99999)%>%select(Latitude, Longitude, Ticket.number)
coordinates(locs) <- ~ Latitude + Longitude
proj4string(locs) <- CRS("+init=esri:102645")
locs <- spTransform(locs, CRS("+init=epsg:4326"))%>%as.data.frame()%>%filter(Latitude < 0)%>%select(real_lng = Latitude, real_lat = Longitude, Ticket.number = Ticket.number)
df <- Parking_Citations%>%filter(!is.na(Latitude), !is.na(Fine.amount), Latitude > 99999)%>%
  left_join(locs)
saveRDS(df, file = "Parking_Citations.rds")
```

```{r eval = FALSE, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}

## Creates locations and a heatmap of data on the map of LA
locs <- Parking_Citations %>%
  filter(!is.na(Latitude), Latitude > 99999) %>%
  select(Latitude, Longitude)
coordinates(locs) <- ~ Latitude + Longitude
proj4string(locs) <- CRS("+init=esri:102645")
locs <- spTransform(locs,CRS("+init=epsg:4326")) %>%
  as.data.frame()
locs2 <- locs[seq(4000000:4100000),]
```

#### Where do parking violations occur?

```{r echo=FALSE, message=FALSE, warning=FALSE}
map <- get_map(getbb("Los Angeles"), zoom = 10, map_type = "roadmap")
ggmap(map) +
  stat_density2d(data = Parking_Citations[4000000:5000000,], aes(x = real_lng, y = real_lat, fill = ..level.., alpha = ..level..), geom = "polygon", size = 0.01, bins = 16) +
  scale_fill_gradient(low = "red", high = "yellow") +
  scale_alpha(range = c(0, 1), guide = FALSE)
```
#### Who receives parking citations?
```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Average fine amount by car Color
Parking_Citations[1:1000, 1:22] %>% 
  filter(!is.na(Fine.amount), !is.na(Color)) %>%
  group_by(Color) %>%
  summarise(avg = mean(Fine.amount), count = n()) %>%
  subset(Color == "RD" | Color == "GY" | Color =="WH" | Color == "BL" | Color == "BK" | Color == "BN") %>% 
  transmute(Color = Color, avg = avg) %>%
  ggplot() + aes(x = Color, y = avg, fill = Color) + geom_histogram(stat = "identity") +
  labs(title = "Average fine amount by car color", x = "Car Color", y = "Average Fine Amount ($)") +
  theme(axis.text.x.bottom = element_text(hjust = 1, angle = 45, size = 8),
        legend.position = "none") +
  scale_x_discrete(labels = c("Black", "Blue", "Brown", "Gray", "Red", "White")) +
  scale_fill_manual(values = c("Black", "Blue", "#8B4513", "Gray", "Red", "White"))
```

```{r holidays, echo=FALSE, message=FALSE, warning=FALSE}
holidays <- c("12/25/2014", "12/25/2015", "12/25/2016", "12/25/2017", "12/25/2018", "12/25/2019", 
              "12/31/2014", "12/31/2015", "12/31/2016", "12/31/2017", "12/31/2018", "12/31/2019", 
              "11/27/2014", "11/26/2015", "11/24/2016", "11/23/2017", "11/22/2018", "11/28/2019",
              "10/31/2014", "10/31/2015", "10/31/2016", "10/31/2017", "10/31/2018", "10/31/2019",
              "06/04/2014", "06/04/2015", "06/04/2016", "06/04/2017", "06/04/2018", "06/04/2019",
              "02/14/2014", "02/14/2015", "02/14/2016", "02/14/2017", "02/14/2018", "02/14/2019",
              "03/17/2014", "03/17/2015", "03/17/2016", "03/17/2017", "03/17/2018", "03/17/2019",
              "02/02/2014", "02/01/2015", "02/07/2016", "02/05/2017", "02/04/2018", "02/03/2019")
holiday_names <- data.frame(date= c("02/14", "03/17", "06/04", "10/31", "11/22", "11/23", "11/24", "11/26","11/27", "11/28", "12/25", "12/31", "02/02", "02/01", "02/07", "02/05", "02/04", "02/03"), 
                            name = c("Valentine's", "St. Patricks", "July 4th", "Halloween", rep("Thanksgiving", 6), "Christmas", "New Years", rep("Super Bowl Sunday", 6)))

h_parking <- Parking_Citations%>%filter(Issue.Date %in% holidays)%>%
              mutate(date = format(as.Date(Issue.Date, format = "%m/%d/%Y"), format="%m/%d"))%>%
              left_join(holiday_names, by = 'date')


h_parking <- h_parking%>%group_by(name)%>%summarise(mean = mean(Fine.amount), sum_prop = sum(Fine.amount)/1965659, count = n()) 

grid.newpage()
vp <- viewport(height = 4, width = 10, default.units = "cm")
pushViewport(vp)
grid.circle(x = rep(seq(1, 9, length.out = 4), 2), y = c(rep(0.5, 4), rep(3.5, 4)), r = h_parking$sum_prop, default.units = "cm", vp = vp)
grid.text(h_parking$name, x = rep(seq(1, 9, length.out = 4), 2), y = c(rep(0, 4), rep(4, 4)), default.units = "cm", vp = vp, gp = gpar(cex = 0.5))

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
## creates sql weather comparison graph
library(RSQLite)
dcon <- dbConnect(SQLite(), dbname = "data.sqlite")
dbListTables(dcon)

mydf <- dbSendQuery(conn = dcon, "
SELECT *
FROM LA_Weather;
") %>%
  dbFetch(-1) %>%
  filter(PRCP >= .5)
dbDisconnect(dcon)

test <- Parking_Citations%>%
  filter(as.Date(Parking_Citations$Issue.Date, format = "%m/%d/%Y")
         %in%
          as.Date(mydf$DATE))

map <- get_map(getbb("Los Angeles"), zoom = 10, map_type = "roadmap")
ggmap(map) +
  stat_density2d(data = test, aes(x = real_lng, y = real_lat, fill = ..level.., alpha = ..level..), geom = "polygon", size = 0.01, bins = 16) +
  scale_fill_gradient(low = "red", high = "yellow") +
  scale_alpha(range = c(0, 1), guide = FALSE)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Average fine by Make of car 
Parking_Citations %>% 
  filter(!is.na(Fine.amount)) %>%
  group_by(Make) %>%
  summarise(avg = mean(Fine.amount), count = n())%>%
  filter(count > 10000) %>%
  ggplot() + aes(x = Make, y = avg) + geom_histogram(stat = "identity") +
  theme(axis.text.x.bottom = element_text(hjust = 1, angle = 45, size = 8))

  
#Average fine by Make of car no graph  
Parking_Citations %>% 
  filter(!is.na(Fine.amount)) %>%
  group_by(Make) %>%
  summarise(avg = mean(Fine.amount), count = n())%>%
  filter(count > 10000)%>% transmute(Make = Make, avg = avg)

#Average fine amount by year
Parking_Citations %>% 
  filter(!is.na(Issue.Date), !is.na(Fine.amount)) %>%
  mutate(year = substring(Issue.Date, 7, 10))%>%
  group_by(year) %>%
  summarise(avg = mean(Fine.amount), count = n())%>%
  filter(count > 20000)%>% 
  ggplot() + aes(x = year, y = avg) + geom_histogram(stat = "identity") +
  theme(axis.text.x.bottom = element_text(hjust = 1, angle = 45, size = 8))


```














##### References:
1. https://la.streetsblog.org/2010/12/13/density-car-ownership-and-what-it-means-for-the-future-of-los-angeles/  
2. https://data.lacity.org/A-Well-Run-City/Parking-Citations/wjz9-h9np  
3.











